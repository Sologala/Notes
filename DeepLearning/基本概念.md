# 数据流

## **前向传播网络**

### deep feedforward network(深度前馈网络)

### multilayer perceptron (多层感知器)

前向传播网络用于设置我们的数据流向，数据经过我们的权值也就是（`Hidden layer`） 之后应该得到的结果。

![eOb4IK.jpg](https://s2.ax1x.com/2019/08/10/eOb4IK.jpg)

对于 **隐藏层** 的`维度` 也就是 整个神经网络的 `depth` 


$$
y_{m\times 1} = x^\top_{n\times1} \times w_{n\times n}\times v_{n\times m}
$$

## 去线性化

一般来说我们的神经网络的`feed` 传入神经网络中，无论有多少 `hidden layer` 我们做的矩阵乘法 都是一个线性的计算，因此输出的结果也是线性的，这样的话我们还不如将所有的**隐藏层** 都舍弃，直接使用输入到输出的映射。因此我们可以使用多层的网络可以模拟任何想要的一种函数，可以更加好的去逼近数据真实的一种分布情况。

### 常用的去线性化的函数

#### 1.ReLU  (Rectified line unit)

**ReLU** 是常用的默认的推荐的激活函数，简单快捷。
$$
g(z) = max(0,z)
$$
![](https://i.loli.net/2019/08/10/e7bSYdMvW4XENcV.png)

可以发现任何小于`0`的值都被过滤掉了，那么所有的数据都被分成了两段，就已经失去原来线性特性了 。

#### 2.Sigmod

$$
g(z) = \frac{1}{1+e^{-x}}
$$

![](https://i.loli.net/2019/08/10/FbtAsgVaWEw2JOH.png)

**sigmod**函数的倒数有如下的性质
$$
sig^{’}(x) = sig(x)(1-sig(x))
$$
任何线性的在`-∞ ~~ ∞` 之间的数据都会被映射到 `0 ~~ 1` 之间。



#### 3.tanh

$$
g(z) = \frac{e^x - e^{-x}}{e^{-x}+e^x}
$$

![](https://i.loli.net/2019/08/10/qij7Jpt3WTQRXaU.png)

**tanh** 的函数也会把整个实数空间都映射到 `-1 ~~ 1` 之间 类似上面的 **sigmod** 

### 解决异或 (XOR) 问题

|  0   | 0    | 0    |
| :--: | ---- | ---- |
|  1   | 0    | 1    |
|  0   | 1    | 1    |
|  1   | 1    | 0    |

异或的结果与我们的输入是不能简单的使用一个线性组合得到，因此我们需要使用一些去线性化的函数来解决这个问题。

## 代价函数（Loss）

对于现在的神经网络已经有了可以将一组`feed`的数据，通过向前传播，并且得到最终的输出。对于我们一般用于深度学习的数据都有带有正确结果的标签的。也就是我们希望得到的输出。

我们可以使用代价函数来评估我们的输出与我们的正确结果之间的**差距**，通过这个差距。

### 常用的代价函数

#### 1.Cross-entropy（交叉熵)

​	
$$
j(\theta) = -reduce\_mean() 
$$
